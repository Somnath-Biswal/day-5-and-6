{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np#matrix math\n",
    "import cPickle as pickle# import load\n",
    "import gym#allows you to train agents based on the game world\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparametrs\n",
    "h = 200# no of hidden neurons\n",
    "batch_size =  10# how many episodes we want in parametrer update\n",
    "learning_rate = 1e-4# learning rate is very smalkl for convergence\n",
    "gamma = 0.99 # discount factor opitimizing words for short term\n",
    "decay_rate = 0.99# gradient descwent\n",
    "# we re gonaa do this by hand\n",
    "resume = False# do we wanna start from prevous model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init model\n",
    "D = 80*80# input dimensionality # intialize aour model as 80*80 pixel value\n",
    "# make it bigger to make pong bigger\n",
    "if resume:# if we want to load from a previous data\n",
    "    model = pickle.load(open('save.p', 'rb'))\n",
    "else:\n",
    "    model = {}\n",
    "    # intialize their weights\n",
    "    # smarter xavier intialization \n",
    "    # hidden node into account when intialization\n",
    "    model['W1'] = np.random.randn(H, D)/np.sqrt(D)# smatrerv than random but actually random for tboth \n",
    "    # interval is from H to D divided by sqrt(D)\n",
    "    # safely this formula\n",
    "    # makes sure the weights are not too small or not too big to propagate the errors\n",
    "    # like vainishing gradient problem\n",
    "    # we do this for both set of weights\n",
    "    model['W2'] = np.random.randn(H)/np.sqrt(H)\n",
    "    # model is going to be a key value pair\n",
    "    # key is going to be construct weights or layer the value should be weights inside it\n",
    "    # the input has been converted into some vector\n",
    "# intialise two more values\n",
    "# gradient buffer and rms_prop_cash\n",
    "# using buffers for gradients and rms_prop intializtaion for backprop storing intermediate values for gradients and computation for actual rms eqaution\n",
    "grad_buffer = { k : np.zeros_like(v) for k,v in model.iteritems() }# for our gradient buffer we have a collection of zeroes and for each key value pair int he model # we should put the model in dictionary\n",
    "rmsprop_cache = { k : np.zeros_like(v) for k,v in model.iteritems()}# gradient buffer  is a way for to store gradients using backprop\n",
    "#it is going to store the value of the formula in the rmsprop_cache \n",
    "# life is stochastic\n",
    "# fate is determinstic static number\n",
    "# variational autoencoders is not stochastic\n",
    "# optimal batchsize to train ann - gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "cannot assign to comparison (Temp/ipykernel_16360/4091712437.py, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\BISWAL\\AppData\\Local\\Temp/ipykernel_16360/4091712437.py\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    I = [ I==144 ] = 0# THIS IS THE TYPE OF THE BACKGROUND $ WE ERASE TO BACKGROUND\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m cannot assign to comparison\n"
     ]
    }
   ],
   "source": [
    "# activation function\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))# squagshing\n",
    "\n",
    "# then there is a preprocessing step\n",
    "def prepro(I):\n",
    "    # convert game train it into just what we need - the padding , the paddles , the ball\n",
    "    I = I[35:195]# in general we see trial and error some subsample of image \n",
    "     # we need to crop it\n",
    "    I = I[::2 , ::2  , 0]# DOWNFACTOR BY A FACTOR OF 2\n",
    "    I = [ I==144 ] = 0# THIS IS THE TYPE OF THE BACKGROUND $ WE ERASE TO BACKGROUND\n",
    "    I = [I==109]=0#THE OTHER BACKGROUND IS BEEN ERASED\n",
    "    I[I !=0 ] =1 # PADDLES AND BALLS SET TO 1\n",
    "    return I.astype(np.float).ravel()                    # we have to return the value as the flattened one dimensional array     \n",
    "\n",
    "# we are optimizing for the short term and the way we do this by implementing this strategy called \n",
    "# we aregoing to recieve a set of rewards from a bunch of timestamps\n",
    "# eevrytime our ball goes through one  direction , it goes past .... our game resets but the score doesnt .. we get rewards for everytime it is happening\n",
    "# Weigh the each of the rewards differently\n",
    "def discount_reward(r):   \n",
    "    # An array of values that is going to feed that into the discount rewards\n",
    "    # weigh of each of rewards differntly .. weigh the most immediate rewards higher than the next rewards\n",
    "    discounted_r = np.zeros_like(r)# it is gonna start off like matrix of zeroes\n",
    "    # discount reward formula\n",
    "    # we have a discount rate and what we are doing weiging the rewards that are immediate , higher and next higher exponentially cause pong is a fast paced game\n",
    "    # for every timestep the rewards are decresing in value of weight \n",
    "    # we are optimizing for the short term and we dont know what happen in long term\n",
    "    # we will start off by intializing the running average or the reward sum and that will store the sum of the rewards \n",
    "    running_add = 0\n",
    "    for t in reversed(range(0,r.size)):# for all values in the range of 0 to r.size\n",
    "        if r[t] !=0:\n",
    "            running_add =0\n",
    "        # time to increment the sum\n",
    "        # the sum over a set of values\n",
    "        running_add = running_add + gamma + r[t]#discount reward factor - this helps us actually weigh those values in exponentially  decreasing order of time\n",
    "        discounted_r = running_add \n",
    "    return discounted_r# short term means whenever ball is going past the imaging thats a short term reward continue hit the ball and there is a reward to all the actions 30 mins ago\n",
    "    # lstm are great for text generation\n",
    "\n",
    "def policy_forward(x):# foreard propagation chain of lineear algebra of matrix multipilcation all those output are fed to next layer get output probabilyy which is either up or down\n",
    "    # then use that sample then use reward value to update our weights recursively that is why called policy gradients \n",
    "    # that we recieve from reward value\n",
    "    # poilicy in the envoirment\n",
    "\n",
    "    # get the input for pixels\n",
    "    h = np.dot(model['W1'],x)\n",
    "    # it gives a vector of values and squash into a linear of values\n",
    "    h[h<0]=0 # relu # relu is used in mid of network while sigmoid is used for the end of network\n",
    "    logp = np.dot(model['W2'], h)# dot product of next set of probablities and the hidden stste\n",
    "    p = sigmoid(logp)\n",
    "    # thrn we want to sample those probabilities and sample them to decide what we want to do\n",
    "    # output probabilites for up down\n",
    "    # sample from prob and decide what we want o do and get the policies by taking gradients wrt to the gradient values\n",
    "    return p,h # return probability of taking action2 and hidden state\n",
    "def policy_backward(eph,epdlogp):\n",
    "    # recursively compute error derivatives for both of those layers\n",
    "    # epdlogp - is going to modulate the event with advantage\n",
    "    # we computing with weight 2 to start off with\n",
    "    # eph is an array of hidden intermediate states\n",
    "    # the ball passes the agent its considered a game and several games and episodes\n",
    "\n",
    "    dW2 = np.dot(eph.T,epdlogp).ravel()\n",
    "    dh = np.outer(epdlogp,model['W2'])\n",
    "    dh[eph<=0]=0\n",
    "    dW1 = np.dot(dh.T,epx)\n",
    "    # return both derivatives to update weights\n",
    "    return {'W1':dW1 , 'W2':dW2}\n",
    "    # llongest commomn subsequence using ml .. sequence standard rnn not something like lsstm\n",
    "    # deep learning on mulrtim deimensional\n",
    "    # linera dimregreesion using sqaush dimensionality reduction technique  , like pca or tsme\n",
    "    # takimg values atevery lasyer and using those value to compute values\n",
    "\n",
    "# implemtnaion\n",
    "env = gym.make('Pong-v0')\n",
    "observation = env.reset()# we were going to take an obvservation from the enviorment \n",
    "# i like hardcoding things and when it comes to pixel values and game values i have never seen before , i have never worried about hardcoding things\n",
    "# extract an pixel, observation from a gameset\n",
    "prev_x = None\n",
    "# we cant calculate motion if the frames are static , we an improve the motion of the ball\n",
    "# we have to improve the motion of our ball if the frames are static\n",
    "# we have to calculate the difference between the two frames and the difference is what we seen there\n",
    "# we have to storet the previous value of diiff\n",
    "# some intermediate variables to store the obseveation hidden sate gradient and reward\n",
    "xs , hs, dlogps , drs = [], [],[], []\n",
    "running_award = None\n",
    "reward_sum = 0\n",
    "episode_number = 0\n",
    "# hyperparametrs dont affect the models architecture , they are used for computing the policy of the reward  not he actual reward itself\n",
    "\n",
    "\n",
    "# begin traininng\n",
    "# it training time\n",
    "while True:\n",
    "    # for training , we' re going to preprocess the information\n",
    "    # get frame difference and for that we need the prev and curr frame\n",
    "    cur_x = prepo(observation)# use the function that has been defined above\n",
    "    x = cur_x - prev_x if prev_x is not None else np.zeros(D)\n",
    "    prev_x = cur_x # now the prev_x is equal to curr_x\n",
    "    #forwards\n",
    "    aprob , h = policy_forward(x)# we need this policy forward method to sample from forward probability and the hidden state\n",
    "    # sample from that probability then do back prop\n",
    "    #stochastic\n",
    "    action = 1 if np.random.uniform() < aprob else 3\n",
    "    # sampling value is stochastic\n",
    "    # stochasticity is in the model reparametriztrion technique\n",
    "    dlog.append(y-aprob)# gradient that encorages that action that is taken to be taken\n",
    "\n",
    "    # step the enviorment\n",
    "    env.render()\n",
    "    observation , reward , done , info = env.step(action)\n",
    "    # standard the open ai format\n",
    "    # we get the observationm , we get the reward , we get the info , binary value , the ball finished before\n",
    "    reward_sum += reward# reward sum is going to store all the rewards\n",
    "\n",
    "    drs.append(reward)# now we are gonna record the reward that has to be done after recall step after getting reward for the prevous action\n",
    "\n",
    "    if done:# means the episode has finished then increment the episode number\n",
    "        episode_number+=1\n",
    "        \n",
    "\n",
    "        # stack inputs hidden states actions rewards for the episodes compute the discounted reward back from\n",
    "        epx = np.vstack(xs)\n",
    "        eph = np.vstack(hs)\n",
    "        edplogp = np.vstack(drs)\n",
    "        # all those rewarsd into their respective stacks\n",
    "        xs , hs , dlogps , drs = [],[],[],[]\n",
    "\n",
    "        # discounted rewards computation\n",
    "        discounted_epr = discount_reward(epr)# store the discounted reward in this variable epr\n",
    "        # help control gradient estimator variance standarise the reward \n",
    "        discounted_epr -= np.mean(discounted_reward)# we take out the average value from these rewards\n",
    "        # then we will average them all together \n",
    "        discounted_epr /= np.std(discounted_epr)\n",
    "\n",
    "        # we are using this discounted reward to compute the gradient value\n",
    "        epdlogp = discounted_epr\n",
    "        # handcode pixels\n",
    "        # pixel value for callls\n",
    "        # we care about ball we care about palace\n",
    "        \n",
    "         # magnitiude has increased in 1 direction still we will be picking from either stoichasticlaly or determeinnstaclly still we 're going to pick one most likely\n",
    "         # advantage says how good this action is compared to al;l the actions \n",
    "         # epdlogp is our gradient and disocunted epr is gonna act as our advantage\n",
    "         #  the way is simple matrix multiplication \n",
    "         # this kinmd of reward of probality is backward denviorment \n",
    "         # final gradients to update weights by using rms weights\n",
    "         # weird math operations \n",
    "         # the formula for rms prop \n",
    "         # rms prop is stochastic gradient descent\n",
    "         # rms prop is the set of eqautons\n",
    "         # we get gradient value and use the decay rate we backprop upadte ove rtime the values that we update closer to r=time are more than values update lesser in time\n",
    "         # decy rate is related to siscount reawrd \n",
    "         # key steo update model key valuess\n",
    "         # \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "06e7043eb1ab438f097d3ca998a94e636d84ae689d7cd0c823c8aef211a80e60"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
